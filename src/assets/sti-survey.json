[
    {
        "$schema": "./sti-survey.schema.json",
        "author": "Hignette",
        "year": 2007,
        "title": {
            "text": "An Ontology-Driven Annotation of Data Tables",
            "link": ""
        },
        "conference-journal": "WISE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Datatypes for each column are collected. The approach uses features of values defined in the ontology for numeric types to recognize the type of numeric columns, and the terms from the taxonomies of the symbolic types to recognize the type of symbolic columns. Column titles are also used to compute the score and choose the type of the column.",
            "type-annotation": "The approach links each cell in symbolic columns to its corresponding type in the ontology, using cosine similarity between weighted vectors. A set of candidate types is defined and a proportion is used to obtain the definitive cell type. Numeric cells' type is obtained by assigning a score to each possible numeric type, according to the cell content, and choosing the highest scoring.",
            "predicate-annotation": "The approach identifies relations by computing a similarity score for each predicate/relation in the ontology according to the table title and the column types.  ",
            "datatype-annotation": "The approach uses a set of manually defined rules to identify each cell's datatype as numeric, symbolic or unknown. A column is classified as symbolic if there are more cells classified as symbolic than numeric, numeric otherwise.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "web table",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": true
    },
    {
        "author": "Hignette",
        "year": 2009,
        "title": {
            "text": "Fuzzy Annotation of Web Data Tables Driven by a Domain Ontology",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "similarity"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Datatypes for each column are collected; the approach computes a score of each symbolic type of the ontology for the column. This score is a combination of the score of the type for the column according to the column contents and the score of the type for the column according to the column title.",
            "type-annotation": "The approach links each cell in symbolic columns to its corresponding type in the ontology, using cosine similarity between weighted vectors. A set of candidate types is defined and a proportion is used to obtain the definitive cell type. Numeric cells' type is obtained by assigning a score to each possible numeric type, according to the cell content, and choosing the highest scoring.",
            "predicate-annotation": "The approach identifies relations by computing a similarity score for each predicate/relation in the ontology according to the table title and the column types. ",
            "datatype-annotation": "The approach uses a set of manually defined rules to identify each cell's datatype as numeric, symbolic or unknown. A column is classified as symbolic if there are more cells classified as symbolic than numeric, numeric otherwise.",
            "entity-linking": {
                "description": " ",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web table (XML)",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Tao",
        "year": 2009,
        "title": {
            "text": "Automatic hidden-web table interpretation, conceptualization, and semantic annotation",
            "link": ""
        },
        "conference-journal": "DKE",
        "name-of-approach": "TISP ++ (Table Interpretation with Sibling Pages)",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Limaye",
        "year": 2010,
        "title": {
            "text": "Annotating and Searching Web Tables Using Entities, Types and Relationships",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach defines a vector of features composed by TFIDF cosine similarity between label in the header and label of concept",
            "predicate-annotation": "The approach defines a vector of features related to the presence and the frequency of the relation between the entities [C/R]",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "The approach define a vector of features composed by different similary measure, TFIDF cosine similarity, Jaccard between cell content and label of the entity [c]",
                "candidate-generation": "lookup, YAGO catalog",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "It mesures the compatibility of the annotations using a new set features: inverse document frequency (IDF), similarity/distance between E and T."
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web Table",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2010,
        "title": {
            "text": "T2LD: Interpreting and Representing Tables as Linked Data⋆",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "T2LD",
        "main-method": {
            "type": "sup",
            "technique": "SVM"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "KBs are queried (e.g. Wikitology, DBpedia) to identify the best classes for each column",
            "predicate-annotation": "The approach generates a set of candidate relations from the relations that exist between the strings in each row of the columns. ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "An additional query to Wikitology is submitted to retrieve the proper entity, using the class labels from the column as additional evidence. The top N entities are returned; a feature vector is created for each cell and the vectors are ranked using a SVM-rank classifier. Then, the highest scoring feature vector is enriched and a second SVM classifier decides whether to link the table cell to the top ranked entity.",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "Wikitology",
                "index": ""
            }
        },
        "output-format": "N3",
        "checked-by-author": false
    },
    {
        "author": "Syed",
        "year": 2010,
        "title": {
            "text": "Exploting a Web of Semantic Data for Interpreting Tables",
            "link": ""
        },
        "conference-journal": "WSC",
        "name-of-approach": "Wikitology",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "The column with the 'title' label is assumed as subject",
            "column-analysis": "",
            "type-annotation": "The class for the column is obtained from the entities class",
            "predicate-annotation": "A set of relations between a pair of entities is returned by Wikitology. The relation that appears most often in the column is selected.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Each instance in the table is mapped to the title and redirect field in Wikitology using an IR index based on Lucene (= the title of Wikipedia articles and redirects). The table header is mapped to types from YAGO, DBpedia, Freebase, WordNet. Entity mention and column header mapped to the firstSentence field (first sentence in Wikipedia). In this way, WIkitology returns the top N possible Wikipedia entities for each string",
                "candidate-generation": "lookup, Wikitology",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Wikipedia tables",
            "kg": {
                "triple-store": "Wikitology",
                "index": "Lucene for concepts"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2011,
        "title": {
            "text": "Automatically Generating Government Linked Data from Tables",
            "link": ""
        },
        "conference-journal": "AAAI",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network + PGM"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The algorithm determines the class for a table column based on the class of the individual strings in the column [C]",
            "predicate-annotation": "The relation between entities that gets majority vote is chosen as the relation between the columns [R]",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Using the predicted class labels for column, the algorithm collect entities. For every entities they create a feature vector, which are ranked using a SVM Rank classifier [C]",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": "These feature vectors are ranked using an SVM-Rank classifier. If the evidence is not strong enough, it suggests that the table cell represents a new entity not present in the \\ac{kg}, which is useful for discovering new entities in the table."
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "Human in the loop"
        },
        "validation": "Custom GS, 15 tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV, XML",
            "kg": {
                "triple-store": "DBpedia,Freebase,WordNet,Yago",
                "index": ""
            }
        },
        "output-format": "N3",
        "checked-by-author": false
    },
    {
        "author": "Venetis",
        "year": 2011,
        "title": {
            "text": "Recovering Semantics of Tables on the Web",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "Using the SVM method we were able to identify the subject column in 94% of the tables.",
            "column-analysis": " ",
            "type-annotation": "The column label is retrieved based on the isA database (instance, class). The boundaries of potential class label in the text are obtained by using the TnT POS tagger; To find the best class for a column, they maximize the probbaility of the values given the class label for the column.",
            "predicate-annotation": "Given two columns, they look for corresponding pairs of values in the columns. If the relation R is extracted for many rows, then R is a likely relation represented by the column. They use TextRunner to extract triples to build a relation database. ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "To create a gold standard for labeling tables, a random sample of tables was taken. Tables without any class or relation labels assigned by the Majority algorithm with a permissive labeling threshold of 10% (R10) were removed. Tables with incorrectly identified subject columns or lacking meaningful concepts were manually eliminated. The remaining 168 tables were presented to human annotators along with the labels generated by the R10 algorithm"
        },
        "validation": "Custom GS",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Goel",
        "year": 2012,
        "title": {
            "text": "Exploiting Structure within Data for Accurate Labeling using Conditional Random Fields",
            "link": ""
        },
        "conference-journal": "ICAI",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "dependent",
            "type": "Weather forecast, flight status and geocoding"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Strings are split at whitespaces and the resulting tokens are split so that the resultant parts are purely alphabetic, numeric or a single simbol character.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach uses a contitional random field CRM to learn the labeling function, usign a graph structure of the features of every token. To predict the most likely label assignment to the field nodes they use Viterbi Algorithm.",
            "predicate-annotation": "The accuracy also increases when we exploit the relationship between the field labels and the token labels.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Knoblock",
        "year": 2012,
        "title": {
            "text": "Semi-automatically Mapping Structured Sources into the Semantic Web",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach uses a contitional random field CRF to learn the labeling function. It uses a feature vector that characterizes the syntactic structure of the column name and the value.",
            "predicate-annotation": "Using the concept annotation, it creates different nodes. For each pair of nodes, it extracts the relation between them from the KG. To identify the right predicate,they use a Stiner Tree Algorithm for computing the minimal tree.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "The user can select the correct annotation using the UI. The system  considers the changes oparated by the users."
        },
        "validation": "",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Data sources Ontologies Semantic Types (training dataset)",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "Source model (used to generate RDF)",
        "checked-by-author": false
    },
    {
        "author": "Pimplikar",
        "year": 2012,
        "title": {
            "text": "Answering Table Queries on the Web using Column Keywords",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "probabilistic graphical model"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "A unified approach based on graphical models to jointly label columns is proposed. Each column has to be labeled with one of the keywords in the query; a similarity score is computed between the keywords and the header tokens/the context of the table. The TF-IDF weighted cosine similarity is computed between the query term and the header. The sum of the soft-maxed match reliability of each term in the query weighted by the TF-IDF score of the term is the similarity between the term and the rest of the table.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Wang",
        "year": 2012,
        "title": {
            "text": "Understanding Tables on the Web",
            "link": ""
        },
        "conference-journal": "ER",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "pattern matching"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "For each column, a concept is derived based on the assumption that entities in a column should belong to the same concept. A candidate schema is generated according to the header of each column; for each schema in the candidate schema list, they enumerate every column and compute a confidence score (they use Probase)",
            "type-annotation": "For a given concept, Probase is used to find its entities and its attributes and viceversa",
            "predicate-annotation": "pattern includes two main factors: the relation between entity and concept, and the relation between attribute and concept.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "It uses the Probase taxonomy which is a probabilistic taxonomy. Column type is used to filter candidates",
                "candidate-generation": "pattern matching",
                "entity-disambiguation": "features"
            },
            "nil-annotation": "we can expand Probase by incorporating this new evidence in the same way as we encounter a new Hearst's pattern."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Custom, 200 wikipedia tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Buche",
        "year": 2013,
        "title": {
            "text": "Fuzzy Web Data Tables Integration Guided by an Ontological and Terminological Resource",
            "link": ""
        },
        "conference-journal": "IEEE",
        "name-of-approach": "ONDINE",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": "microbial risk, chemical risk, aeronautics"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Distinction between symbolic and numerical columns",
            "type-annotation": "A score for each column is computed leveraging: 1. The column title; 2. The column content (computed as term similarity between the terms in the column and the terms in the OTR). wrt the concepts that appear in the signature of the relations belonging to the OTR.",
            "predicate-annotation": "Each relation in the OTR is scored, using a proportion of simple concepts in the signature of the relation which are represented by columns in the table.",
            "datatype-annotation": "Symbolic and numerical columns are distinguished, using knowledge in an Ontological and Terminological Resource (OTR) i.e., unit concepts. ",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Custom, 90 tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Domain specific web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "XML documents representing data tables; RDF annotations",
        "checked-by-author": false
    },
    {
        "author": "Cruz",
        "year": 2013,
        "title": {
            "text": "GIVA: A Semantic Framework for Geospatial and Temporal Data Integration, Visualization, and Analytics",
            "link": ""
        },
        "conference-journal": "SIGSPATIAL",
        "name-of-approach": "Giva",
        "main-method": {
            "type": "sup",
            "technique": "AgreementMaker"
        },
        "domain": {
            "domain": "dependent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "They translate the data into a common spatial data format.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "They use string matching on the column header and perform random sampling on the values to find pattern similarities (to identify the column that contains spatial data).",
            "type-annotation": "",
            "predicate-annotation": "Geospatial classification schemes can be modeles using a part-of or is-a relationship",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "GML, KML, Shapefile, MapInfo TAB, HTML table",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Deng",
        "year": 2013,
        "title": {
            "text": "Scalable Column Concept Determination for Web Tables Using Large Knowledge Bases",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "lookup",
        "main-method": {
            "type": "unsup",
            "technique": ""
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "A similarity score between the column and the identified types is computed. They extend the MapReduce-based similarity joins to support scalable column concept determination. 1. For each column, the list of types that share at least one entity/cell  value with the column; 2. Compute top-k types of every column; 3. Compute the overlap similarity. Both exact-matching and fuzzy-matching similarity functions are employed.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web",
            "kg": {
                "triple-store": "DBpedia,Freebase,Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ermilov",
        "year": 2013,
        "title": {
            "text": "User-driven Semantic Mapping of Tabular Data",
            "link": ""
        },
        "conference-journal": "I-SEMANTICS",
        "name-of-approach": "",
        "main-method": {
            "type": "",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "The RDF mapping obtained from Sparqlify-CSV a page is created for each resource on the mappings wiki, which uses the Semantic MediaWiki extension to allow for semantic annotations over the content of such pages. "
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV, TSV, XLS, XLSX",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2013,
        "title": {
            "text": "Semantic Message Passing for Generating Linked Data from Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Acronyms are expanded and stylized literal values (e.g. phone numbers) are recognized",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "The query and rank module identifies a list of possible candidate assignments using data from DBpedia, YAGO and Wikitology. The column potential classes are the union of the classes of its cells (from DBpedia and YAGO).",
            "type-annotation": "The initial entities assigned to a column's cell values perform a majority voting over the YAGO and DBpedia class set to pick the top YAGO and DBpedia class. Each entity votes and increments the score of a class; the YAGO and DBpedia sets are then ordered and the top classes are considered. Each class is assigned a confidence score; if the score is below a certain threshold, the column header is assigned to NO-ANNOTATION",
            "predicate-annotation": "The approach uses the links between pairs of entities to generate candidate relations. For a pair of cell values in the same row between two columns, the candidate entity sets for both cells are obtained. For each possible pairing, YAGO and DBpedia are queried to obtain relations in either direction. Also, relations between columns are considered.",
            "datatype-annotation": "We use a regular expression to distinguish string mentions, which probably refer to entities, and literal constants such as numbers and measurements, which probably do not. If the cell value is a literal constant,candidate entities are not generated and the cell is mapped to no-annotation.",
            "entity-linking": {
                "description": "A set of candidate entities for each cell is generated using Wikitology (which uses DBpedia, Wikipedia and YAGO). An entity ranker re-ranks a cell's candidate entities using an approach mutuated from literature. ",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Web Manual, Web Relation, Wiki Manual, Wiki Links",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web and Wikipedia HTML tables",
            "kg": {
                "triple-store": "DBpedia,Yago,Wikitology",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Munoz",
        "year": 2013,
        "title": {
            "text": "Triplifying Wikipedia's Tables",
            "link": ""
        },
        "conference-journal": "LD4IE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "exact match"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Each page Wikipedia containing HTML tables is cleaned and canonicalized before extracting tables fixing syntax mistakes using CyberNeko",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "DBpedia is queried for relationships that exist between entities in cells of the same row. All pairs are queried. An additional query is performed to find the relation between entities in a row and the subject of the page the table belongs to. They look for relations that hold in either direction.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Each link in a cell is mapped to a DBpedia entity URI by following redirects and replacing namespace http://en.wikipedia.org/wiki/ to http://DBpedia.org/resource/",
                "candidate-generation": "no candidate since the purpose is to extract rdf triples",
                "entity-disambiguation": "redirects"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "They extract 250 triples from the set and manually annotate them to produce a GS to test the approach on.",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Wikipedia tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Quercini",
        "year": 2013,
        "title": {
            "text": "Entity Discovery and Annotation in Tables",
            "link": ""
        },
        "conference-journal": "EDBT",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Spatial data are broken down into their components via the Google Geocoding API. To train the multiclass classifier they convert snippets to lowercase and tokenize. Then they remove stopwords and stem the reimainders with the Porter algorithm. Each token is then associated with its normalized frequency",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Cells are scanned so as to eliminate those that are not likely to contain the names of entities of a given type. They look at the syntactic properties of the contents of each cell. They use regex to identify patterns of e.g. phone numbers, URLs, emails, or leverage GFT types to rule out the cells containing a specific type of information that is not necessary to identify a pre-determined type (e.g. cells that match the email pattern are excluded when identifying cells belonging to the restaurant type). ",
            "type-annotation": "They perform type (concepts in ontology) annotation at cell-level, so that each column can contain more than one type. Once the table has been annotated, based on the assumption that data types in a column are homogenous in well-formed tables, incorrect annotations are discarded",
            "predicate-annotation": " ",
            "datatype-annotation": "regex",
            "entity-linking": {
                "description": "The approach first identifies row containing information on a specific type (obtained from an ontology) then determines the cells that contain the names of those entities. The multiclass classifier is trained on a training dataset (of snippets of positive examples of a pre-defined type) by: 1. Creating a set of entities from DBpedia belonging to categories that are manually identified as useful; 2. Collecting snippets from Bing; 3. 75% of the snippet corpus is training set, 25% is the test set.  After the text classifier (either SVM or Naive Bayes in experiments) has classified cells, spurious annotations are removed. ",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "Information about unknown entities is found on the Web so as to annotate them with the correct type. They use the content of the cell to query the Web and use the obtained results (which are links to Web pages+snippet) to determine whether the cell contains the name of an entity of a certain type. The result of a query is a snippet and they use a text classifier to determine whether the snippet describes an entity of a certain type. "
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Google Fusion Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zhang",
        "year": 2013,
        "title": {
            "text": "InfoGather+: Semantic Matching and Annotation of Numeric and Time-Varying Attributes in Web Tables",
            "link": ""
        },
        "conference-journal": "SIGMOD",
        "name-of-approach": "InfoGather+",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": "We assume that the set of conversion rules are known up- front. They are specified by the system administrator (or domain experts) using a simple, rule specification language. Each rule has 3 components: a left-hand side (LHS), a con- version factor (θ) and a right-hand side (RHS). The LHS and RHS are strings describing units and scales. For exam- ple, for the rule Euro = 1.3 × USD, the LHS is Euro, θ is 1.3 and the RHS is USD. Figure 3(a) shows more examples of rules."
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "They infer label information from semantically matching columns of other Web tables. Given a set of pre-defined rules and: Case 1: a set of unit/scale descriptors (eg USD, EUR etc), they annotate the column with the value if it (or one of its synonyms) either appears in the header or column values. Case 2: a set of years strings, they annotate the column with the value if it occurs in the header or in the context of the table.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": ""
        },
        "validation": "Experiments conducted on three real-life datasets of web tables,extracted from a recent snapshot of Microsoft Bing search engine",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "EAB (entity-attribute binary relationships) HTML Web tables ",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zwicklbauer",
        "year": 2013,
        "title": {
            "text": "Towards Disambiguating Web Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "majority voting",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Subset of Limaye",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "DBpedia",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Sekhavat",
        "year": 2014,
        "title": {
            "text": "Knowledge Base Augmentation using Tabular Data",
            "link": ""
        },
        "conference-journal": "LDOW",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "probabilistic model"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "patty patterns",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "ClueWeb09 dataset",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "",
            "kg": {
                "triple-store": "",
                "index": "Yago"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2014,
        "title": {
            "text": "A Scalable Approach to Learn Semantic Models of Structured Sources",
            "link": ""
        },
        "conference-journal": "IEEE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "For each attribute in the data source, a set of candidate sematnic types (with confidence level) is collected and ranked. To learn semantic types, they use a supervised machine learning technique based on Conditional Random Fields (CRF) with features extracted from the attribute names and sample data from the new source. Type = ontology class or data-property + domain-class pair",
            "predicate-annotation": "To learn relationships between types, the authors leverage previously built semantic models belonging to the same domain (e.g. museums) assuming that similar sources will share similar semantic models. To annotate relationships, they use a directed weighted graph 𝐺 built on top of the known semantic models and expanded using the semantic types 𝑇 and the domain ontology 𝑂; properties are weighted links between nodes. The algorithm used to construct 𝐺: 1. Adds known semantic models: each model is added if it is not a subgraph of already added components; 2. Adds semantic types, checking for each type if the graph contains a match for it; 3. Adds paths from the ontology, using the subclass hierarchy in the ontology",
            "datatype-annotation": "They assign a class to attributes whose values are URIs and assign a domain/data property pair to attributes containing literal values (see Type/concept annotation column)",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "We applied our approach on this dataset to find the candidate semantic models for each source and then compared the best suggested models with models created manually by domain experts"
        },
        "validation": "We evaluated our approach on a dataset of 29 museum data sources",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Multiple sources modeled using the EDM, AAC, SKOS, Dublin Core Metadata Terms, FOAF, ORE, and ElementsGr2 ontologies",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Bhagavatula",
        "year": 2015,
        "title": {
            "text": "TabEL: Entity Linking in Web Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "TabEL",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "TabEl performs EL in three steps: 1. Mention identification; 2. Entity candidate generation; 3. Disambiguation. It also relies on a prior estimate that a given string refers to a particular entity, using an estimate distribution from hyperlinks on the Web and in Wikipedia.  The number of in-links to an entity in Wikipedia is an indicator of its prominence",
                "candidate-generation": "lookup, YAGO",
                "entity-disambiguation": "features, probabilistic"
            },
            "nil-annotation": "TabEL demonstrates strong performance in identifying and disambiguating unlinked mentions in Wikipedia tables. Unlike previous experiments that removed all existing links, TabEL for this task retains the existing links."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "TabEL",
        "code-availability": "http://websail-fe.cs.northwestern.edu/TabEL/ ",
        "license": "CCA 4.0",
        "inputs": {
            "type-of-table": "Web table (XML)",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ramnandan",
        "year": 2015,
        "title": {
            "text": "Assigning Semantic Labels to Data Sources",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "SemanticTyper",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "For textual data, the authors use the TF-IDFbased approach and for numeric data, they use the Kolmogorov-Smirnov (KS) statistical hypothesis test. In order to resolve column heterogeneity the authors adopted the rule that in the training data, if for a semantic label the fraction of pure numeric data values is below 60%, it is trained as textual data (and hence indexed as document). If the fraction of numeric values is above 80%, it is trained as purely numeric data (its distribution is extracted to be used in KS test) after discarding textual data values. In the other case (if the fraction is between 60% and 80%), the data is trained as both textual and numeric data (it is both indexed as a document and its ",
            "type-annotation": "TF-IDF",
            "predicate-annotation": " ",
            "datatype-annotation": "Welch's t-test",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": ""
        },
        "validation": "we used data from the museum domain consisting of 29 data sources in diverse formats from various art museums in the U.S. Semantic labels",
        "code-availability": "https://github.com/usc-isi-i2/eswc-2015-semantic-typing",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "",
                "index": "training data with Lucene, not KG data"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ritze",
        "year": 2015,
        "title": {
            "text": "Matching HTML Tables to DBpedia",
            "link": ""
        },
        "conference-journal": "WIMS",
        "name-of-approach": "T2K Match",
        "main-method": {
            "type": "unsup",
            "technique": "feature"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "remove HTML artifacts, special characters and additional whitespaces. Further, value lists are split into individual values, all values are lower-cased and normalized. For the normalization, we use a set of handcrafted transformation rules to resolve abbreviations, e.g. “co.” is transformed into “company”. Moreover, the header is detected",
                "spell-checker": "",
                "units-of-measurements": "Further, we normalize units of measurements using around 200 manually generated conversion rules, e.g. 8mi2 is converted to 20.72 million m2."
            },
            "subject-detection": "",
            "column-analysis": "We further perform a data type detection for each attribute which is important for choosing the similarity measure. The data type is detected using about 100 manually defined regular expressions. They are able to detect string, numeric value, timestamp and coordinate. The final data type decision for an attribute is based on majority vote.",
            "type-annotation": "we determine the distribution of DBpedia classes of the best candidate for each entity and choose the most frequent classes as candidates for schema matching.",
            "predicate-annotation": "Each value of the top candidates votes for a correspondence between the attribute and its property. This vote is weighted by the value-based similarity of the two values and the similarity value from the candidate selection step. Votes from all values are summed up and the attribute property pair with the highest value is chosen. It follows the intuition that a similar attribute property pair has many similar values on similar entities/candidates. Our computation is purely duplicate-based and does not perform any label matching on the headers since headers in HTML tables do not often have meaningful names. The matching of attributes with properties is further aggregated by summing up all the scores of all property correspondences per class to refine the class ranking. At this point we choose the class with the highest score as final correspondence.",
            "datatype-annotation": "We further perform a data type detection for each attribute which is important for choosing the similarity measure. The data type is detected using about 100 manually defined regular expressions. They are able to detect string, numeric value, timestamp and coordinate. The final data type decision for an attribute is based on majority vote.",
            "entity-linking": {
                "description": "First, we search for the entity label in DBpedia. The found candidates are ranked according to a similarity function and the top k candidates are kept. Then, we determine the distribution of DBpedia classes of the best candidate for each entity and choose the most frequent classes as candidates for schema matching. In turn, this is used to refine the candidate resources: All candidates not belonging to a chosen class are removed, and for each entity we perform another search with the selected classes as additional constraint. We compute similarities between the values of HTML table entities and candidate resources by applying two blocking strategies: (1) the values of each entity are only compared to the values of its candidates and (2) only values with the same data type are compared. In case of multi-values, we calculate the similarity of all combinations and choose the maximum.",
                "candidate-generation": "lookup, DBP LS",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": "The article explicitly mentions that with the T2K approach one could fill the missing values to DBpedia and vice-versa (Web Tables). Howver, they do no provide any evidence on how they store or use new entities / properties / values in DBpedia. "
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D",
        "code-availability": "https://github.com/olehmberg/T2KMatch",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Web Table (HTML)",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ermilov",
        "year": 2016,
        "title": {
            "text": "TAIPAN: Automatic Property Mapping for Tabular Data",
            "link": ""
        },
        "conference-journal": "EKAW",
        "name-of-approach": "TAIPAN",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "After characterizing columns by means of their support and connectivity scores, we can use binary classifiers to classify columns of a table as being either subject columns or not. Best classifiers SVM, Decision Tree",
            "column-analysis": " ",
            "type-annotation": "Approach takes into account the relation between the S-column and the other NE columns. It uses a probabilistic model based on the most frequent types in the \\ac{kg} and majority voting to determine the column type annotation.",
            "predicate-annotation": "relation probability: number of matching pairs / number of table rows",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Improved T2D",
        "code-availability": "https://github.com/dice-group/TAIPAN",
        "license": "GPL 3.0",
        "inputs": {
            "type-of-table": "Not specified, they mention only tables. DBpedia Table Dataset (DBD)",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Neumaier",
        "year": 2016,
        "title": {
            "text": "Multi-level semantic labelling of numerical values",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Hierarchical clustering (BKG)"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "Only numerical values.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "by splitting the DBpedia data into a test and training dataset.",
        "code-availability": "https://github.com/sebneu/number_labelling",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Pham",
        "year": 2016,
        "title": {
            "text": "Semantic labeling: A domain-independent approach",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "DSL(Domain-independent SemanticLabeler)",
        "main-method": {
            "type": "sup",
            "technique": "Logistic Regression"
        },
        "domain": {
            "domain": "independent",
            "type": "Musum, city, soccer, weather"
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "In our approach, we compute three different value similarity metrics: Jaccard similarity and TF-IDF cosine similarity for textual data, as well as a modified version of Jaccard similarity for numeric values. For numeric data, there are semantic types that we are unable to distinguish by using value similarity because they have the same range of values. Therefore, we analyze the distribution of numeric values contained in the attributes using statistical hypothesis testing as one of the similarity metrics. We use KolmogorovSmirnov test (KS test)[6] as our statistical hypothesis test based on evaluation of different statistical tests in Ramnandan et al.'s research ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "In our approach, we compute three different value similarity metrics: Jaccard similarity and TF-IDF cosine similarity for textual data, as well as a modified version of Jaccard similarity for numeric values.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D",
        "code-availability": "https://github.com/minhptx/iswc-2016-semantic-labeling",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "The input of our system is an unlabeled attribute and a set of labeled attributes as domain data.",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "The output is a set of top-k semantic types corresponding to the unlabeled attribute.",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2016,
        "title": {
            "text": "Learning the semantics of structured data sources",
            "link": ""
        },
        "conference-journal": "JOWS",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "Cosine similarity + statistical hypothesis"
        },
        "domain": {
            "domain": "dependent",
            "type": "museum"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "syntactic information about data sources such as attribute names or attribute types (string, int, date, ...) may give the system some hints to discover semantic types. We employ the technique proposed by Ramnandan2015 to learn semantic types of source attributes. Their approach focuses on learning the semantic types from the data rather than the attribute names. It learns a semantic labeling function from a set of sources that have been manually labeled. ",
            "type-annotation": "For each class node v in sm(si), we search the graph to see if G includes a class node with the same label. It is possible that sm(si) contains multiple class nodes with the same label, for instance, a model including the link isFriendOf from one Person to another Person. In this case, we make sure that G also has at least the same number of class nodes with that label. Once we added the required class nodes to the graph, we map the class nodes in the model to the class nodes in the graph. If G has multiple class nodes with the same label, we select the one that is tagged by larger number of known semantic models. We add anentry to H with v as the key and the mapped node(v′) as the value.",
            "predicate-annotation": "The goal is to connect class nodes of G using the direct paths or the. paths inferred through the subclass hierarchy in O. Once we apply this labeling method, it generates a set of candidate semantic types for each source attribute, each with a confidence value. Our algorithm then selects the top k semantic types for each attribute as an input to the next step of the process. 11 ,...,tp1k Thus, the output of the labeling step for s(a1,a2,...,am) is T = {(tp11 1k ),...,(tpm1 m1 ,...,tpmk mk )}, where in tpij ij , tij is the jth semantic type learned for the attribute ai and pij is the associated confidence value which is a decimal value between 0 and 1.",
            "datatype-annotation": "If the data values associated with a source attribute ai are textual data, the labeling algorithm uses the cosine similarity between TF/IDF vectors of the labeled documents and the input document to predict candidate semantic types. For attributes with numeric data, the algorithm uses statistical hypothesis testing [25] to analyze the distribution of numericvalues.",
            "entity-linking": {
                "description": "Therefore, we adopt a different strategy; if there is more than one node in the graph matching a node in the semantic model, we select the one having moretags.",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "Not only entities. When adding a new node or link, we tag it with a unique identifier (e.g., si, name of the source) indicating that the node/link exist in sm(si)."
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "Users then interact with the system to adjust the automatically generated model. During this process, users can transform the data as needed to normalize data expressed in different formats and to restructure it."
        },
        "validation": "Museum data, eventhough they say that the use some tables of museum data from the gold standard it does not mention which GS they use. The link to the datasets https://github.com/taheriyan/jws-knowledge-graphs-2015",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "XML files and JSON files. They can also import the domain ontologies they want to use for modeling the data. The system then automatically suggests a semantic model for the loaded source.",
            "kg": {
                "triple-store": "CIDOC-CRM,EDM",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2016,
        "title": {
            "text": "Leveraging Linked Data to Discover Semantic Relations within Data Sources",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "Auhtors exploit the predicates used by LOD datasets, by extracting all predicates that connect two given classes in the LOD. BANKS algorithm computes the top k minimum cost trees that span a subset of the nodes in a graph (the nodes that the semantic types are mapped to).",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "GUI usage"
        },
        "validation": "",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV, XML and JSON",
            "kg": {
                "triple-store": "CIDOC-CRM",
                "index": ""
            }
        },
        "output-format": "a semantic model expressing how the assigned labels are connected",
        "checked-by-author": false
    },
    {
        "author": "Efthymiou",
        "year": 2017,
        "title": {
            "text": "Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "hybrid",
            "technique": "Hybrid"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "stopword removal",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "the label column (that I think refers to the subject column) is defined as the leftmost column with the maximum number of distinct (non-numeric) values",
            "column-analysis": "column sampling",
            "type-annotation": "",
            "predicate-annotation": "associate each column with an object property having as domain the ontology class to which the subject column entity belongs",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "(a) a lookup-based method which relies on the minimal entity context provided in Web tables to discover correspondences to the KB, (b) a semantic embeddings method that exploits a vectorial representation of the rich entity context in a KB to identify the most relevant subset of entities in the Web table, and (c) an ontology matching method, which exploits schematic and instance information of entities available both in a KB and a Web table.",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "entity embedding, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "TD2, Limaye, Wikipedia (a GS constructed by the authors created by extracting the hyperlinks of existing Wikipedia tables to Wikipedia pages, which we have replaced with annotations to the corresponding entities from the October 2015 version of DBpedia.) Since the header rows in Wikipedia tables are not linked to properties, our gold standard does not contain schema-level mappings.",
        "code-availability": "http://www.cs.toronto.edu/~oktie/webtables/",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "JSON",
        "checked-by-author": false
    },
    {
        "author": "Ell",
        "year": 2017,
        "title": {
            "text": "Towards a Large Corpus of Richly Annotated Web Tables for Knowledge Base Population",
            "link": ""
        },
        "conference-journal": "LD4IE",
        "name-of-approach": "This paper is not about an apporach rather than on proposing techniques or hypothesis about the tables in order to improve STI approaches performance",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Table Normalization Task (normalizing values found in cells); format of the dates; regular expersion to detect a certain language",
                "spell-checker": "",
                "units-of-measurements": "Values, such as those representing weights, lengths, volumes, time etc. can have unit identifiers attached. For each string that appears to be a value followed by a unit identifier we create a hypothesis that contains both the value and the base unit identifier separately where the value is converted to the base unit"
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "Given the value of plain hypotheses created by table normalization, we check the three indexes related to the language the hypothesis is based on for all resources this value could refer to and order the results by their frequency. For the top 10 entities, properties, and classes we create cell-based hypotheses.",
            "predicate-annotation": "",
            "datatype-annotation": "This task makes use of an index for each language (containing language-tagged strings as well as datatyped-literals from the respective language version of DBpedia) to quickly retrieve a set of properties given an entity, a literal, and a datatype.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "features"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "WDC Web Table Corpus 2015",
        "code-availability": "",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Web Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": "Labels + literals"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zhang",
        "year": 2017,
        "title": {
            "text": "Effective and Efficient Semantic Table Interpretation using TableMiner+",
            "link": ""
        },
        "conference-journal": "JOWS",
        "name-of-approach": "TableMiner+",
        "main-method": {
            "type": "unsup",
            "technique": "probabilistic features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "BoW representation, stopword removal",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "Subject column detection is done by using simple regular expressions that examine the syntactic features of cell text, such as number of words, capitalization, mentions of months or days in a week ('empty', 'named entity', 'number', 'date expression', 'long text' (e.g., sentence, or paragraph), and 'other'). Next, if a candidate NE-columnhasacolumnheader that is a preposition word, it is discarded.",
            "column-analysis": "generates candidate concepts for an NEcolumn and computes confidence scores for each candidate. Intuitively, if we already know the entity annotation for each cell, we can define candidate concepts as the set of concepts associated with the entity from each cell. Specifically, in each iteration, a cell taken from the column is disambiguated by comparing the feature representation of each candidate entity against the feature representation of that cell. Then the concepts associated with the highest scoring (i.e., the winning) entity7 are gathered to create a set of candidate concepts for the column.",
            "type-annotation": "it performs NE annotation by coupling column classification with entity disambiguation in an incremental, mutually recursive, bootstrapping approach. The NE annotation starts with a LEARNING phase that is able to interpret independently an NE column to create preliminary concept annotation for an NE-column and entity annotation for the cells of the column. The cells with the same content are treated as a singleton in order to build a shared and combined context in-table. The number of features in the representation of each cell is used as a preference score. Such is followed by the update phase, which revises the annotations iteratively by enforcing interdependence between columns.",
            "predicate-annotation": "Relation enumeration firstly begins by interpreting relations between the subject column and any other columns on each row independently. Then a confidence score is computed for each candidate relation. the subset of triples containing as predicate are selected. Then the object of each triple in this set is matched against the content in the table using the frequency weighted dice function and the highest score is assigned to be the confidence score for the candidate relation. The dice function computes an overlap score between the bag-of-words representations of the cell and the object of a triple that contains. The confidence score is comuted and it is composed of a relation instance score re and a relation context score.",
            "datatype-annotation": "Literal-columns are expected to contain attribute data of entities in the subject column. This work also assigns a column annotation that best describes the attribute data in literal-columns. Given a literal-column Tj that forms a binary relation rj,jwith the subject column Tj, the annotation for this column is simply l(rj,j), since rj,jtypically describes a property of the subject column concept in such cases.",
            "entity-linking": {
                "description": "entity linking and the NE column identification and annotattion occure in the same time",
                "candidate-generation": "lookup",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "For evaluation, we compiled and annotated four datasets using Freebase: Limaye200, LimayeAll, IMDB and MusicBrainz.",
        "code-availability": "https://github.com/ziqizhang/sti",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "Freebase",
                "index": "rdf graph"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Kacprzak",
        "year": 2018,
        "title": {
            "text": "Making Sense of Numerical Data - Semantic Labelling of Web Tables",
            "link": ""
        },
        "conference-journal": "EKAW",
        "name-of-approach": "NUMER",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "strip non numerical chars from numeric columns",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "In this work we focus on labelling columns with numerical data, assuming a subject column has been previously identified.",
            "column-analysis": "considers as numeric the columns with at least 50% of numerical values, else the columns are classified as textual.",
            "type-annotation": "",
            "predicate-annotation": "",
            "datatype-annotation": "Compare the distribution of values in numerical columns with bags of values from the target KB. They consider only columns that have a semantic relation with the types of the entities in the subject column. For each retrieved type from the KG, a list of all instances in the target KB is generated. For each entity they select properties of rdf:type owl:DatatypeProperty associated to it.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "MLL",
        "code-availability": "https://github.com/chabrowa/semantification",
        "license": "MIT",
        "inputs": {
            "type-of-table": "",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Luo",
        "year": 2018,
        "title": {
            "text": "Cross-Lingual Entity Linking for Web Tables",
            "link": ""
        },
        "conference-journal": "AAAI",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "neural network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "we first generate candidate entities of each single mention, then we learn two different features: the mention feature and context feature derived from the mention-entity embedding pairs of the table. To make different representations from two language spaces compatible. Meanwhile, we learn a third feature called coherence feature only from the candidate entity table. We train word embeddings and entity embeddings on two corpus of different languages separately. The vector spaces of embeddings in different languages are naturally incompatible, and it's hard for us to directly compare or calculate them. To tackle this problem, we employ a bilingual translation layer to map embeddings from one language space to another.",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "we attempt to solve the cross-lingual table linking problem without using any non-English knowledge bases. The paper presents a joint statistical model to simultaneously link all mentions that appear in one table. The framework is based on neural networks, aiming to bridge the language gap by vector space transformation and a coherence feature that captures the correlations between entities in one table. Experimental results report that our approach improves the accuracy of cross-lingual table linking by a relative gain of 12.1%. Detailed analysis of our approach also shows a positive and important gain brought by the joint framework and coherence feature."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "Wikipedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zhang",
        "year": 2018,
        "title": {
            "text": "Ad Hoc Table Retrieval using Semantic Similarity",
            "link": ""
        },
        "conference-journal": "WWW",
        "name-of-approach": "STR",
        "main-method": {
            "type": "unsup",
            "technique": "neural network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Bag of entities",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "Core = Subject. We introduce a simple and effective core column detection method. It is based on the notion of column entity rate, which is defined as the ratio of cells in a column that contain an entity. ",
            "column-analysis": "",
            "type-annotation": "",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "We employ a fielded entity representation with five fields (names, categories, attributes, similar entity names, and related entity names) and rank entities using the Mixture of Language Models approach. The field weights are set uniformly. This corresponds to the MLM-all model in and is shown to be a solid baseline.We return the top-k entities, where k is set to 10.",
                "candidate-generation": "lookup, SPARQL",
                "entity-disambiguation": "entity embedding"
            },
            "nil-annotation": "a method for performing semantic matching between queries and tables. The \"raw\" content of a query/table is represented as a set of terms, where terms can be eitherwords or entities. Each of the raw terms is mapped to a semantic vector representation"
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "TabEL",
        "code-availability": "https://github.com/iai-group/www2018-table",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Chabot",
        "year": 2019,
        "title": {
            "text": "DAGOBAH: An End-to-End Context-Free Tabular Data Semantic Annotation System",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "DAGOBAH",
        "main-method": {
            "type": "hybrid",
            "technique": "Rule Base & Embeddings"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "DAGOBAH conducts a preliminary cleaning process is first applied in order to have a macroscopic transformation process covering the most known artefacts: encoding homogenization and special characters deletion (parenthesis, square bracket and non alphanumeric characters) optimize the lookups. The intent is not to correct every string issues, but to",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "The key column is an Object column containing a large number of unique values and located on the left side of the table.",
            "column-analysis": "Discriminate Columns with String, DateTime, Numerical",
            "type-annotation": "A basic type coverage of the cells criteria is not relevant as right types might be more specific but not frequent enough to be consider as the target ones. To solve this issue, a threshold based on relative scores is used. To each type t in {ti} (list of all types in a given column), a score St is first associated, from which a relative score Rt is computed. Only types with Rt > 0.7 (configurable threshold) are considered in the next steps. In order to select the target type from the short-listed ones, a TF-IDF-like method is used to compute the specificity.",
            "predicate-annotation": "Concerning the CPA, a simple lookup technique on the header was used during round 1 explaining the low accuracy.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "In order to enhance the CEA results, the previous ordered types are used to disambiguate the candidates entities. If the first candidate of the lookup has the target type, it is selected as the target entity; if not, we select the entity associated to this type in the lookup list (if the list is empty, no annotation is produced). Both regex and Levenshtein distance strategies have been implemented.",
                "candidate-generation": "lookup, WKD API, Wikidata Cirrus Engine, DBP LS, Wikipedia API, custom index",
                "entity-disambiguation": "features, entity embedding"
            },
            "nil-annotation": "Challenges: a high dependency on lookup services (over which DAGOBAH has little control) and difficulties in correctly setting up algorithms (in particular finding the right compromise between specificity and representativeness of types in the case of CTA). Concerning the CPA, a simple lookup technique on the header was used during round 1 explaining the low accuracy."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "SemTab2019",
        "code-availability": "",
        "license": "Orange",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Chen",
        "year": 2019,
        "title": {
            "text": "ColNet: Embedding the Semantics of Web Tables for Column Type Prediction",
            "link": ""
        },
        "conference-journal": "AAAI",
        "name-of-approach": "ColNet",
        "main-method": {
            "type": "hybrid",
            "technique": "CNN"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "1. lookup. Given an entity column, it retrieves column cells' corresponding entities in the KB and adopts the classes of the matched entities as a set of candidate classes for annotation. 2. prediction. For each candidate class, a customized binary CNN classifier which is able to learn both inter-cell and intra-cell locality features is trained and applied to predict whether cells of a column are of this class. 3. ensemble. Cell to entity matching and voting with majority can contribute to a highly confident prediction, while prediction with CNNs, which considers the contextual semantics of words can deal with type disambiguation and recall cells missed by lookup.",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "In sampling, we first retrieve a set of entities from the KB, by matching all the column cells with KB entities according to the entity label and entity anchor text using a lexical index. Those matched entities are called particular entities. The classes and super classes of each particular entity are inferred (via KB reasoning) and they are used as candidate classes for annotation, denoted as C. The reason of selecting candidate classes instead of using all the KB classes is to avoid additional noise, thus reducing false positive predictions and computation. For each candidate class, we further infer all of its KB entities that are not matched. They are defined as general entities. The second round uses each column's candidate classes from the first round to constrain the cell to entity. matching, thus refining the entity suggestions. This step filters out some particular entities and candidate classes with limited matching confidence",
                "candidate-generation": "lookup",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": "ColNet is a framework that utilizes a KB, word representations and machine learning to automatically train prediction models for annotating types of entity columns that are assumed to have no metadata.. It first embeds the overall semantics of columns into vector space and then predicts their types with a set of candidate KB classes, using machine learning techniques like Convolutional Neural Networks (CNNs) and an ensemble of results from lookup. To automatically train robust prediction models, we use the cells to retrieve the candidate KB classes, infer their entities to construct training samples, and deal with the challenge of sample shortage using transfer learning. In summary, this study contributes a more accurate column type annotation framework by combining knowledge lookup and machine learning with the knowledge gap considered. As the framework does not assume any table metadata or table structure, it can be applied to not only web tables but also general tabular data. The study also provides a general approach that embeds the overall semantics of a column where the correlation between cells is incorporated."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "The method is evaluated with DBpedia and two different web table datasets, T2Dv2 from the general Web and Limaye from Wikipedia pages",
        "code-availability": "https://github.com/alan-turing-institute/SemAIDA",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "NE and entity linking",
        "checked-by-author": false
    },
    {
        "author": "Chen",
        "year": 2019,
        "title": {
            "text": "Learning Semantic Annotations for Tabular Data",
            "link": ""
        },
        "conference-journal": "IJCAI",
        "name-of-approach": "ColNet",
        "main-method": {
            "type": "unsup",
            "technique": "CNN"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "focuses on annotating columns that consist of phrases. For instance, a column containing \"Google, Amazon and Apple Inc.\" can be annotated by the class Company. To achieve this, they propose a method called HNN that captures the contextual semantics of a column.",
            "predicate-annotation": "Property features are used to represent the potential relations between the target column and its surrounding columns. Property Vector algorithm is proposed for property annotation.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D, Limaye, Efthymiou",
        "code-availability": "https://github.com/alan-turing-institute/SemAIDA ",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Cremaschi",
        "year": 2019,
        "title": {
            "text": "MantisTable: an Automatic Approach for the Semantic Table Interpretation",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "MantisTable",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "aims to clean and uniform data inside the table. Transformations applied to tables are as follows: deletion of HTML tags and some characters (i.e.), transformation of text into lowercase, deletion of text in brackets, resolution of acronyms and abbreviations, and normalisation of units of measurement.",
                "spell-checker": "To decipher acronyms and abbreviations, the Oxford English Dictionary3 is used.",
                "units-of-measurements": "The normalisation of units of measurement is performed by applying regular expressions. MantisTable extends the original set of regular expressions to cover a complete set of units, which includes. area, currency, density, electric current, energy, flow rate, force, frequency, fuel efficiency, information unit, length, linear mass density, mass, numbers, population density, power, pressure, speed, temperature, time, torque, voltage and volume."
            },
            "subject-detection": "subject column detection that takes into account the identified NE-columns. We can define the S-column as the main column of the table based on different statistic features, like Average Number of Words (aw) in each cell, Fraction of Empty Cells (emc) in the column, Fraction of Cells with Unique content (uc) and Distance from the First NE-column (df). The column with the highest score will be selected as the S-column for the considered table.",
            "column-analysis": "whose tasks are the semantic classification that assigns types to columns that are named entity (NE-column) or literal column (Lcolumn), and the detection of the subject column (S-column). The first step of the Column Analysis phase is to identify good L-column candidates",
            "type-annotation": "For each winning entity (Eij) we pick the associated concepts (rdf:type), then for each column we build a dictionary Cj where each rdf:type have associated the number of rows of the table containing that concept divided by the total number of entities found for that column. We use a threshold set at 40% of the maximum score, the concepts with a score lower then this threshold are discarded. Table 4 show an example of this scoring. With the concept list obtained in this way, we build a concept graph representing the hierarchy between them. in order to complete the column annotation task we pick the path of the winning connected component which maximizes the score.",
            "predicate-annotation": "Predicate Annotation, whose task is to find relations in the form of predicates, between the Subject column and the other columns, to set the overall meaning of the table. MantisTable approach considers these to fpredicates found in the Entity Linking phase and takes the predicate with maximum frequency ",
            "datatype-annotation": "To accomplish this task, we consider 16 regular expressions that identify several Regextypes (e.g. numbers, geo coordinate, address, hex color code, URL). If the number of occurrences of the most frequent Regextype in a column exceeds a given threshold, that column is annotated as L-column, otherwise, it is annotated as NE-column. To distinguish between three main datatypes: dates, numbers and strings. While dates and strings are matched exactly, numeric matching is done using anapproximated matching algorithm: a pair of numbers makes a match if the distance (the absolute difference) between the two is less than a threshold.",
            "entity-linking": {
                "description": "To discover the mappings for each cell in a row we take a set of eligible entities from the KG. entities are considered eligible if label contains the cell's content or contains the cell's tokens. By converting this information to a graph representation we find all the paths that, starting from a eligible entity of the cell's subject, enter into a eligible entity of the cell's object of the row. To increase the accuracy we also try to match the literal cells with the literals in the subject cell's candidate entities: that is, we seek for a match between literals in the table and eligible entities by using a simple matching algorithm.",
                "candidate-generation": "lookup, SPARQL",
                "entity-disambiguation": "features"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": ""
        },
        "validation": "Challenge rounds",
        "code-availability": "https://bitbucket.org/disco_unimib/mantistable-tool.py/src/master/",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Hulsebos",
        "year": 2019,
        "title": {
            "text": "Sherlock: A Deep Learning Approach to Semantic Data Type Detection",
            "link": ""
        },
        "conference-journal": "SIGKDD",
        "name-of-approach": "Sherlock",
        "main-method": {
            "type": "sup",
            "technique": "Neural network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "computation of statistic features",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "multi-input deep neural network",
            "predicate-annotation": "",
            "datatype-annotation": "multi-input deep neural network",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D",
        "code-availability": "https://github.com/mitmedialab/sherlock-project ",
        "license": "MIT",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Kruit",
        "year": 2019,
        "title": {
            "text": "Extracting Novel Facts from Tables for Knowledge Graph Completion",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "TAKCO",
        "main-method": {
            "type": "hybrid",
            "technique": "PGM + features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": true
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "selecting the column with most unique non-numeric values breaking ties by choosing the leftmost one. For every cell in the key column, we then select a set of entity candidates.",
            "column-analysis": "",
            "type-annotation": "",
            "predicate-annotation": "We can now compute likelihood scores for mapping cells to relations (Eq.4), and for mapping columns to relations",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "compute a score of the row-entity assignments by comparing all cell values in the row with all the labels of entities that are connected to the candidate entities. Authors use jaccard to compute the highest attainable string similarity between the cell at column c and row ρ and the values of the r-links from e. Then the row-entity assignments which maximise the coherence in the table, i.e., maximise the similarity between the entities are calculated. These assignments are determined using Loopy Belief Propagation (LBP).",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": "Our method does not prune out row-entity assignments, but performs the interpretation by performing inference over all possible assignments. The PGM uses label similarities as priors, and then updates its likelihood scoring to maximise the coherence of entity assignments across the rows using Loopy Belief Propagation (LBP). Coherence is not computed using a predefined metric but is automatically selected as a combination of properties that are shared by the entities in the table. This is a novel feature of our method which makes it capable of working with KGs with different topologies and/or relations. We also propose an approach to perform slot-filling by disambiguating attribute cells in a novel link-prediction framework. Our approach makes use of embeddings of KG entities and relations to improve the quality of the disambiguation whenever label matching is not sufficient. This furthers our aim to find novel facts for KG completion."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2Dv2, Webaroo",
        "code-availability": "https://github.com/karmaresearch/takco",
        "license": "MIT",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "DBpedia,Wikidata",
                "index": ""
            }
        },
        "output-format": "New facts to complete KGs",
        "checked-by-author": false
    },
    {
        "author": "Morikawa",
        "year": 2019,
        "title": {
            "text": "Semantic Table Interpretation using LOD4ALL",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "LOD4ALL",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "In this step, estimating a column type (CTA result) using candidate entities extracted. This step consists of the three following steps: 1- Calculate a cover ratio; 2-Search a class score from the score DB; 3-Calculate a predict score using both a score ratio and a class-score",
            "predicate-annotation": "In this step, we first collect candidate predicates using entities determined in Step 4 by executing SPARQL (Listing 1.1) to RDF store. %URI1% and %URI2% in SPARQL are replaced entities determined in Step 4. Next, by calculating a frequency, we obtain the predicate of the greatest frequency. The output of this step is the result of the CPA task.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "The original literal search function of LOD4ALL uses Elastic- search that returns a subject (a candidate entity) corresponding to an object that matches a cell value. This function can obtain candidate entities in the following steps: Direct search: First, by combining http://DBpedia.org/resource/ with the cell value, we create a candidate entity. Next, we execute ASK query to RDF store. Keyword search: We have enhanced the literal search function of LOD4ALL that can retrieve subjects that have rdfs:label, foaf:name, foaf:surname and foaf:givenName in the triple. We have adopted SimString[8] for the similar string search. We have set 100.0 for the score of an entity found by Direct search, and set a value multiplying the Elasticsearch's score and the SimString's score for the score of an entity that have found by Keyword search.",
                "candidate-generation": "lookup, SPARQL, custom index",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "SemTab2019",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": "ElasticSearch"
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Nguyen",
        "year": 2019,
        "title": {
            "text": "MTab: Matching Tabular Data to Knowledge Graph using Probability Models",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "MTab",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "use of ftfy tool for text Decoding; use of fasttext models for Language Prediction: Data Type Prediction-13 predefined data types of duckling (numerical tags, email, URL, or phone number. If there is no tag assigned, we assign this cell type as a text tag): pre-trained SpaCy models for Entity Type Prediction to predict 18 entity types; If there is no tag assigned, this cell type is assigned to a text tag; Entity Lookup on DBpedia spotlight / endpoint for relevant entities with English URL.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "we categorize table columns to entity columns and literal columns. We aggregate the data type (Duckling Tags and SpaCy Tags) from each cell in a column using majority voting. If the majority tag is text or entityrelated, the columns is an entity column, else a numerical column. Regarding numerical columns, we perform semantic labeling with EmbNum method [5] to annotate relations (DBpedia properties) for numerical columns 9. Then, we infer types (DBpedia classes) from those relations.",
            "type-annotation": "Given a set of entity columns in table S is Ment, we consider these signals (i) from the probabilities of type potential from numerical columns; (ii) the probabilities of type potential aggregated from the types of entity lookup for the all cells in column mj; (iii) the probabilities of type potential aggregated from SpaCy entity type prediction for the all cell in column mj. We used majority voting and normalized these voting value to [0,1]. Then, we associate those normalized voting value type potential probabilities; and (iv) the probabilities of type potential given header value of the column mj. We associate the normalized Levenshtein distance as potential probability that a type (DBpedia class) correspond with a header value.",
            "predicate-annotation": "Given two columns mj1 and mj2 , we estimate the probabilities of relation potential. We consider two type of relation between two columns: Entity column to Entity column and Entity column to non-Entity column.",
            "datatype-annotation": "The set of numerical columns in table S is Mnum. Given a numerical column mj, we use re-trained EmbNum model on DBpedia [5] to derive embedding vector for the list of all numerical values of the column and then search the corresponding relations from the database of labeled attributes 10. The result qmj is a ranking of relevance numerical attributes in terms of distribution similarity. We also use α as the limit for ranking result. For textual values: We use the normalized Levenshtein distance while For numerical values: the relevance ratio.",
            "entity-linking": {
                "description": "Given a cell value ci,j, we have a set of ranking result lists from lookup services. In MTab, we adopted the four services as DBpedia lookup, DBpedia Endpoint, Wikidata lookup, and Wikipedia lookup. However, we can use any services as long as their output is a ranking list of relevance entities.",
                "candidate-generation": "lookup, DBP LS, DBP endpoint, Wikipedia API, WKD API",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "SemTab2019",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "vertical relational table.",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Oliveira",
        "year": 2019,
        "title": {
            "text": "ADOG - Annotating Data with Ontologies and Graphs",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "ADOG",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "Not detailed",
            "predicate-annotation": "Not detailed",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "string similarity",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "Lehvenstein distance"
            },
            "nil-annotation": "the matching process can start by matching the data against the ElasticSearch index. When several matches are returned from the matching process, additional measures are employed to score the relevance of each match to the query, considering the context of the data to annotate. The three main steps are calculating the similarity and frequency of properties measures, and final score weighting. This measure finds the string similarity between query words and the matched terms. Both strings are normalised, punctuation is removed, and word inside brackets are ignored. The similarity measure uses Levenshtein Distance (LD). If any extra properties, besides the labels, were indexed from the source KG, this step calculates and normalises their frequencies for each match. For example, in DBpedia, these properties can be the categories, types or even other entities linked to the matched entity via an object property. The final score of each candidate will be weighted considering the previous steps, plus the normalised ElasticSearch score for each search performed. These weights are variable and can be adjusted to fit any model, giving more or less weight to similarity, search scores, or property frequencies."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "https://github.com/danielapoliveira/iswc-annotation-challenge.",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "ontology, KG and Table",
            "kg": {
                "triple-store": "DBpedia",
                "index": "ArangoDB + ElasticSearch"
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Steenwinckel",
        "year": 2019,
        "title": {
            "text": "CSV2KG: Transforming Tabular Data into Semantic Knowledge",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "CSV2KG",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "For each of the cell values, we first clean them by retaining only the part that comes before a \"(\" or \"[\" and by removing all \"%\", \"\"\", and \"\\\" characters",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "Most specific class that matches the entities in the cell of a column. Since the classes of the DBpedia ontology form a hierarchy, they can be represented in a tree. We can now traverse this tree and apply majority voting (i.e. taking the child with the highest count) on each level of this tree. We continue recursively until the entropy of the two highest counts of its children is lower than a specified threshold.",
            "predicate-annotation": "The cell annotations are used to annotate the properties or relations between pairs of columns. To do this, we iterate over cell pairs from two target columns between which we want to infer the relation. Then, for each of these cell pairs (s, o), we query for all predicates p from the DBpedia ontology that exists between these two entities: {p | (s,p,o) ∈ DBpedia}. Finally, the predicate that can be found the most between the cell pairs is chosen. In case of relationships with equal highest counts, we check the range and domain using the column types. When the range and domain of multiple relationships are valid possibilities, we take the relationships with the most specific range and domain column type (using depth(domain) + depth(range)).",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Then, we check whether http://DBpedia.org/resource/<X> exists where <X> is simply the cleaned cell value with spaces replaced by underscores (try url). Parallel with this, the cell value is provided to the DBpedia lookup API to generate more candidates (DBpedia lookup). As no column type annotations are available yet during this initial phase, we do not provide this additional information. If both the DBpedia lookup and the try url step did not result in any candidates, the DBpedia Spotlight API is applied. In the end, a large pool of possible candidates remain. On this pool, we apply disambiguation by selecting the candidate of which its rdf:label has the lowest Levenshtein distance to the actual cell value.",
                "candidate-generation": "lookup, DBP LS, DBP urls, DBP Spotlight",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": "First, crude annotations are made for each cell in the table by generating multiple candidates and disambiguating them by using string similarities on the cell values and the rdf:label of each candidate. Afterwards, the column types and properties between columns are inferred using these cell annotations. In a fourth step, the inferred column types and properties are used to create more accurate head cell annotations (the cells in the first column of a table). Phase five uses the new head cells to correct the other cells in the table, using the property annotations. Finally, in phase six, new column types were inferred using all the available corrected cells."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2019",
        "code-availability": "https://github.com/IBCNServices/CSV2KG",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Takeoka",
        "year": 2019,
        "title": {
            "text": "Meimei: An Efficient Probabilistic Approach for Semantically Annotating Tables",
            "link": ""
        },
        "conference-journal": "AAAI",
        "name-of-approach": "meimei",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "For literal-columns, we compute several numerical statistics of the column cells",
            "type-annotation": "It is natural that we use the mean (and standard deviation) of embedding vectors as the feature vector of a column given the analogy that the mean of word embedding vectors is often used as the feature vector of a sentence or documents (Arora, Liang, and Ma 2017). The mean of word embedding vectors is often used as the feature vector of a sentence even if embedding approaches based on long shortterm memory appeared. Since the characteristics of columns are similar to those of bag-of-words, we use the mean of embedding vectors as a feature vector.",
            "predicate-annotation": "",
            "datatype-annotation": "Since literal-columns may also have textual information such as measurement units (e.g., “20 s”), we also use several textual features such as the frequency of each letter and the length of each string. Our feature extraction for literalcolumns comprises three steps: 1. Calculate numerical statistics: we extract numerical values from cells xc and calculate several statistics vnum from the values. 2. Calculate textual features: we calculate several textural features vtxt such as the frequency of each letter. 3. Concatenate features: we concatenate numerical statistics vnum with textual features vtxt",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "This paper presents a novel approach for table data annotation that combines a latent probabilistic model with multilabel classifiers. It features three advantages over previous approaches due to using highly predictive multi-label classifiers in the probabilistic computation of semantic annotation. (1) It is more versatile due to using multi-label classifiers in the probabilistic model, which enables various types of data such as numerical values to be supported. (2) It is more accurate due to the multi-label classifiers and probabilistic model working together to improve predictive performance. (3) It is more efficient due to potential functions based on multi-label classifiers reducing the computational cost for annotation. Extensive experiments demonstrated the superiority of the proposed approach over state-of-the-art approaches for semantic annotation of real data (183 human-annotated tables obtained from the UCI Machine Learning Repository)."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Private company dataset",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "WordNet",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Thawani",
        "year": 2019,
        "title": {
            "text": "Entity Linking to Knowledge Graphs to Infer Column Types and Properties",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "We use candidates for all cells in a column to compile a set of all the classes and properties used to describe them, and for each candidate, we define a uniform-length binary sparse feature vector to record the classes and properties used to describe it. These features are not equally informative. We seek to maximize coverage and selectivity. Intuitively, a feature has good coverage if for every cell there is a candidate for which this feature has value 1. TF-IDF is used",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "For each column C our algorithm starts from dbo:Thing on level 0 of semantic class tree and calculates the percentages of cells in C that belong to classes on certain level of the DBpedia class tree. The algorithm then picks most common class with the highest percentage above a given threshold.",
            "predicate-annotation": "In the CPA task, the goal is to find the DBpedia ontology property that best matches the relation between a primary column and other secondary columns. As input, we have the ranked candidates from CEA and sample size N, which is the top N of the candidates we want to consider, to limit the size of our query search space. For each row, we query DBpedia for the properties between all pairs of candidates in the primary and secondary columns.",
            "datatype-annotation": "For secondary columns consisting of literals, we transform the value to address potential differences between the cell value and the values in the KG. Currently we address cases of string, date, and numerical literals.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "External lookup (Wikidata API), custom index",
                "entity-disambiguation": "Lehvenstein distance, contextual inf, NN"
            },
            "nil-annotation": "work we investigated a feature engineering approach. Lexical features capture the lexical similarity between the contents of a cell and entity labels, and semantic features capture semantic coherence among cells in a column."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2019",
        "code-availability": "https://github.com/usc-isi-i2/wikidata-wikifier/blob/master/wikifier/wikifier.py",
        "license": "MIT",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "",
                "index": "ElasticSearch"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zhang",
        "year": 2019,
        "title": {
            "text": "Sato: Contextual Semantic Type Detection in Tables",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "Sato",
        "main-method": {
            "type": "sup",
            "technique": "multi-layer NN + LDA topic modeling"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Remove content in parentheses, convert strings to lowercase, capitalize words except for the first and concatenate results into a single string.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "Combines topic modeling and structure learning with a single-column type prediction based on the Sherlock model. Two types of context are used:\n- Global, the set of all cell values in the table;\n- Local, the set of independently predicted semantic types of the neighboring columns in the same table. The type annotation is achieved using column-wise features including character embeddings, word embeddings, paragraph embeddings as well as column statistics.",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2Dv2",
        "code-availability": "https://github.com/megagonlabs/sato",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "DBpedia",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Abdelmageed",
        "year": 2020,
        "title": {
            "text": "JenTab: Matching Tabular Data to Knowledge Graphs",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "JenTab",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "we use a regular expression to split up terms that are missing spaces like in \"1stGlobal Opinion Leader's Summit\". Similarly, we remove certain special characters like parentheses. The result of these steps are stored as a cell's \"clean value\"",
                "spell-checker": "YES (autocorrect.py), Fix incorrect encoding (ftfy), split terms with missing spaces, remove special chars. Finally, we also apply an off-the-shelf spell checker, autocorrect to fix typos resulting in an \"autocorrected value\" per cell",
                "units-of-measurements": ""
            },
            "subject-detection": "The first column is the Subject (assumption)",
            "column-analysis": "Datatypes (object, date, string, and number). NON SPIEGA COME",
            "type-annotation": "Candidate generation: \n- Retrieve the candidate concepts of the entities found in the CEA considering also the instanceOf and subclassOf relationships Disambiguation:\n- Least Common subsummer: check the concept hierarchy\n- Direct Parents: majority vote\n- Popularity: select the concept by populrity",
            "predicate-annotation": "Retrieve candidate predicates by considering relationships between cells (from subject colum). It considers both object properties and literal properties. For literals, different heuristics are considered for different datatypes. Disambiguation:\n- Majority Vote",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Different strategies for obtaining candidates:\n- Generic: cell value and WD labels\n- Full Cell: lookup cell value\n- All Token: lookup tokens w/o stopword\n- Selective: remove () and then lookup\n- Token: lookup token in isolation\n- Autocorrection: lookup corrected val \n- Context\nDifferent strategies for selecting candidates:\n- String similarity: Levenshtein distance and popularity of entities\n- By column: check the presence of the same value in a column",
                "candidate-generation": "lookup, WKD API",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2020",
        "code-availability": "https://github.com/fusion-jena/JenTab ",
        "license": "MIT",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "Wikidata",
                "index": ""
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Azzi",
        "year": 2020,
        "title": {
            "text": "AMALGAM: making tabular dataset explicit with knowledge graph",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "AMALGAM",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Fix incorrect encoding (Pandas)",
                "spell-checker": "YES (Gurundi.py)",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "After the prelinking using Wikidata API, they count the concept occurrences",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Lookup using Wikidata API. They consider the context (row) and the concept of the column",
                "candidate-generation": "lookup, WKD API",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2020",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "Wikidata",
                "index": ""
            }
        },
        "output-format": "CEA,CTA",
        "checked-by-author": false
    },
    {
        "author": "Baazouzi",
        "year": 2020,
        "title": {
            "text": "Kepler-aSI : Kepler as A Semantic Interpreter",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "KEPLER-ASI",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "- deletion of some chars\n- trasformation into lowercase\n- delete text in brackets\n- normalisation of units",
                "spell-checker": "",
                "units-of-measurements": "Yes (regex)"
            },
            "subject-detection": "- aw: the average number of words in each cell\n- emc: the fraction of empty cells in the column\n- uc: the fraction of cells with unique content\n- df: the distance of the first NE-column",
            "column-analysis": "Regex, eg, numbers, geographic coordinates, address, hexadecimal color code, URL",
            "type-annotation": "The content of a column is fetched in query to Wikidata. If multiple entities were returned for a cell, the one with the number of occurrences was taken.",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2020",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "Wikidata",
                "index": ""
            }
        },
        "output-format": "CTA",
        "checked-by-author": false
    },
    {
        "author": "Chen",
        "year": 2020,
        "title": {
            "text": "LinkingPark: An Integrated Approach for Semantic Table Interpretation",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "LinkingPark",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "Yes (tailor made, the corrector checks all strings within one edit distance to the original mention string)",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "Most common types shared by most of the entities. In case of the same count, prioritise the most specific one.",
            "predicate-annotation": "start from the strings in the cells (in the same row) and generate candidates considering numerical properties and entity properties.",
            "datatype-annotation": "set of statistics per numeric datatypes (range, mean, stdev)",
            "entity-linking": {
                "description": "Candidate Generation: Lookup usign Wikidata API and fine-grained Elastic Search index (word-based, trigram-based) Entity Disambiguation: Type consistency along each column of entities and property relatedness within each row of attribute values TF-IDF weighing",
                "candidate-generation": "lookup, Mediawiki API, custom index",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2020",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "Wikidata",
                "index": "ElasticSearch"
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Cremaschi",
        "year": 2020,
        "title": {
            "text": "A fully automated approach to a complete Semantic Table Interpretation",
            "link": ""
        },
        "conference-journal": "FGCS",
        "name-of-approach": "MantisTable",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Deletion of HTML tags and some characters, transformation of text into lowercase, deletion of text in brackets, explanation of acronyms and abbreviations, and normalisation of units of measurement.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "The following list of features is considered:\n- Fraction of empty cells;\n- Distance from the first NE-column;\n- Average number of words.\nThen, each column is assigned a score and the highest scoring is the S-column",
            "column-analysis": "Semantic classification into either Literal columns (L-column) via regex or NE-column",
            "type-annotation": "For each winning entity, all the rdf:type values are extracted from DBpedia. Then, for each extracted type, the frequency (considering also different ontologies) and the number of cells in which the type appears are calculated. The set of type candidates is defined according to two conditions: (i) fall within the range defined by the maximum global frequency as upper bound and δ as lower bound; (ii) if the type belongs to a number of cells greater than a threshold β. The most specific concept from the two branches of the hierarchy is used to annotate the column.",
            "predicate-annotation": "The winning concept of the S-column is the subject of relationships among columns, and annotations of the other columns are objects. To identify candidate predicates, they exploit two techniques based on exploratory queries and summary profiles. The former searches the KG for the subject and the object with two distinct methods for NE-columns and L-columns, while the latter uses a distributed tool that integrates profiles for RDF data",
            "datatype-annotation": "Datatype annotation relies on the results of the Column Type Analysis that delivered an association between every L-column and a specific Regextype. A mapping between Regextypes and Datatypes is performed. In the case of one-to-one relation, the corresponding datatype is used to annotate the L-column, otherwise a further analysis step is required to identify the correct datatype,",
            "entity-linking": {
                "description": "When the query returns more than one entity, the edit distance (i.e. Levenshtein distance) is computed between the normalised table cell content and the normalised  candidate entity label. Only the entity with the smallest edit distance is considered for the annotation.",
                "candidate-generation": "lookup",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": ""
        },
        "validation": "Semtab2019, T2D, Limaye",
        "code-availability": "https://bitbucket.org/disco_unimib/mantistable-tool/",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Cremaschi",
        "year": 2020,
        "title": {
            "text": "MantisTable SE: an Efficient Approach for the Semantic Table Interpretation",
            "link": ""
        },
        "conference-journal": "SemTab",
        "name-of-approach": "MantisTable SE",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "All the cells of all the tables are analysed using a tokeniser managing special characters and removing parenthesis and the text block inside them.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "The subject column (S-column) can be identified between NE-column thanks to content-based scores",
            "column-analysis": "Semantic classification into either Literal columns (L-column) via regex or NE-column",
            "type-annotation": "Every entity is reconsidered wrt the CPA reordering. Only considering WIkidata, the concepts are retrieved that correspond to every candidate entity from LamAPI. For every column, an in-memory structure storing frequencies (by-max normalization) of the most common datatypes is used. The winning concept is chosen as follows:\n- concepts with a frequency lower than 0.95 are discarded\n- considering every possible column pair, count inbound and outbound edges; the final concept selected is the one having the highest number of connections in the Wikidata graph.",
            "predicate-annotation": "All the predicates are retrieved for every possible candidate couple with confidence greater than 0 and they are sorted by their relative frequency to the entire column. The predicate with the greatest frequency will be ranked first.",
            "datatype-annotation": "Literal columns are identified through regex while all the otherr columns are considered to be NE columns",
            "entity-linking": {
                "description": "For each cell of the S-column and NE-columns a confidence score is calculated by computing the edit distance (Levenshtein) between the labels in different languages of candidate eneity and the content of the cell. L-columns obtain a confidence score that is computed differently for numeric, string and date datatypes.",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Semtab2020",
        "code-availability": "https://bitbucket.org/disco_unimib/mantistable-4 ",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia,Wikidata",
                "index": "LamAPI"
            }
        },
        "output-format": "CEA,CTA,CPA",
        "checked-by-author": false
    },
    {
        "author": "Eslahi",
        "year": 2020,
        "title": {
            "text": "Annotating Web Tables through Knowledge Bases: A Context-Based Approach",
            "link": ""
        },
        "conference-journal": "SDS",
        "name-of-approach": "",
        "main-method": {
            "type": "hybrid",
            "technique": "features & embeddings"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "",
            "type-annotation": "the three most frequent types obtained from an Entity Linking phase serve as a filter to enhance the results. Among these three types, the most frequent type in the column is determined using majority voting as the selection criterion.",
            "predicate-annotation": "",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Candidate Generation Lookup: create list of candate and majority vote on type on whole column Looping Method: cosine similarity between their vector representations (embedding) then build a graph with all unambiguous candates from the table.",
                "candidate-generation": "lookup",
                "entity-disambiguation": "features, entity embedding"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "",
            "description": ""
        },
        "validation": "Limaye, T2D",
        "code-availability": "https://github.com/eXascaleInfolab/sds2020_web_table_annotation",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web table",
            "kg": {
                "triple-store": "Wikidata",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    }
]